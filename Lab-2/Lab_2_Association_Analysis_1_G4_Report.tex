\documentclass[a4paper,10pt]{article}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\setlength\parindent{0pt}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{ragged2e}
\usepackage{listings}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{soul}
\usepackage[font=small,labelfont=bf]{caption}
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%

\begin{document}
\begin{titlepage}
	\centering
	\includegraphics[width=.6\textwidth]{liu-logo.png}\par
	\vfill
	{\scshape\Large TDDD41 Data Mining - Clustering and Association Analysis\par}
	{\huge\bfseries Lab 2 -  Group 4 Report\par}
	\vspace{0.5cm}
    {\large\itshape Lawrence Thanakumar Rajappa (lawra776)\\
     \large\itshape Kyriakos Domanos (kyrdo817)\par}
	\vfill
	{\large \today\par}
\end{titlepage}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{center}
	\large\textbf{\underline{Association Analysis}} \par
\end{center} \par
\textbf{\underline{Data Preparation}} \par
In this lab assignment, we used iris dataset which contains 50 samples of data which corresponds to three species of
iris. As a starting process, we discretize the data since association analysis in weka cannot be done on continuous
data. This is done by applying discretize filter available in Weka on only first 4 columns and now, the entire data
is converted discrete format. \par
\textbf{\underline{Clustering - SimpleKmeans}} \par
Now, we tried to apply \textbf{\textit{SimpleKmeans}} clustering algorithm with the following specifications;
\begin{enumerate}
  \item[$*$] \textbf{Seed value} $:$ 10
  \item[$*$] \textbf{No. of Clusters} $:$ 3 
\end{enumerate}
and igonred \textbf{\textit{class}} attribute and selected Classes to clusters evaluation to crosstabulate the 
clustering. We tried to visualize the result of clustering and found that with 3 bins, we could see that sepal 
width and length have lower impact when compared with petal width and length which has higher impact in naming
the flower types. This is because, sepal width and length try to overlap more than petal width and length. Please
refer the below images,
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{Sepal_dimension_visualization.png}
  \captionof{figure}{Sepal width and length tends to have more overlap in the data}
\end{center}
\par
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{Petal_dimension_visualization.png}
  \captionof{figure}{Petal width and length tends to have less overlap in the data}
\end{center}
and also having less number of bins, we could lose some information as well.
From the clustering result, we could see that clustering has been done well with 3 bins and 3 clusters, \textbf{\textit{iris setosa}}
has been clustered properly and belongs to \textbf{\textit{Cluster 2}}, while there is a swap in case of 
\textbf{\textit{iris versicolor}} and \textbf{\textit{iris virginica}} between \textbf{\textit{Cluster 0}} and
\textbf{\textit{Cluster 1}}.  Please refer to the confusion matrix below, \par
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{kmeans_clustering.png}
  \captionof{figure}{SimpleKmeans clustering with 3 bins}
\end{center}
Also, if we try to increase the number of bins and discretize, the results are getting worse, please refer the 
image below,
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{kmeans_clustering_1.png}
  \captionof{figure}{SimpleKmeans clustering with 5 bins}
\end{center}
\par
\textbf{\underline{Association Analysis - Apriori}} \par
Here we have used the following specifications for performing Apriori algorithm on 3 clustered dataset;
\begin{enumerate}
  \item [$*$] \textbf{Minimum Support} = 0.1
  \item [$*$] \textbf{Metric type} = Confidence
  \item [$*$] \textbf{Minimum Confidence} = 0.9
\end{enumerate}
with the above properties, we have performed apriori algorithm on the iris dataset and got the following association rules
which are given below,
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{apriori_assoociation.png}
  \captionof{figure}{Apriori - Association Rules with Confidence}
\end{center}
In this frequent itemset are the attribute-value pair and transaction is a single data point. Moreover, with 
default properties we weren't able to get proper association rules, hence we changed our properties a bit;
\begin{enumerate}
  \item [$*$] \textbf{Number of Rules} = 1000
  \item [$*$] \textbf{Metric type} = Confidence
  \item [$*$] \textbf{Minimum Confidence} = 0.9
\end{enumerate}
In the above properties, \textbf{\textit{Number of Rules}} which specifies how many rules are needed and we get
1000 rules from this property and we keep the rule with 90\% confidence.
\begin{enumerate}     
  \item [$*$] sepalwidth='(-inf-2.8]' petallength='(2.966667-4.933333]' ==$>$ Cluster=cluster1, Occurrence = 30, Confidence:(1)
  \item [$*$] petallength='(2.966667-4.933333]' petalwidth='(0.9-1.7]' ==$>$ Cluster=cluster1, Occurrence = 48, Confidence:(1)
  \item [$*$] petallength='(4.933333-inf)' petalwidth='(1.7-inf)' ==$>$ Cluster=cluster2, Occurrence = 40, Confidence:(1)
  \item [$*$] sepallength='(6.7-inf)' petallength='(4.933333-inf)' ==$>$ Cluster=cluster2, Occurrence = 17, Confidence:(1)
  \item [$*$] sepallength='(-inf-5.5]' petallength='(-inf-2.966667]' ==$>$ Cluster = cluster3, Occurrence = 47, confidence:(1)
  \item [$*$] sepalwidth='(2.8-3.6]' petallength='(-inf-2.966667]' ==$>$ Cluster=cluster3, Occurrence = 36, Confidence:(1)
\end{enumerate}
\par
From the above association rules, we could see that we want class to be in consequent rather than in antecedent, because
we need to predict the class of the flower and also when a new data comes into the system, we need to predict its species
instead of having the class at beginning and attributes are placed in consequent because it is not to be predicted. \par
\textbf{\underline{Experiments}} \par
\textbf{\underline{Different Number of Bins}} \par
By increasing number of bins, we could yield more information and thus we can calculate distance between data points 
more precisely in k-means algorithm. On the other hand, having to many clusters which would yield less information and
possibly it could make the algorithm ambiguous. 

We tried to discretize the data with \textbf{\textit{6 bins}} and found that the amount of incorrectly clustered
data points are more when compared with the clustering result with 3 bins, which makes the above line true. We then
applied \textbf{\textit{addCluster}} filter on the 6 bins data and found the following association rules;
\begin{enumerate}
  \item [$*$] petallength='(-inf-1.983333]' ==$>$ cluster=cluster1, occurrence = 50    conf:(1)
  \item [$*$] petalwidth='(-inf-0.5]' ==$>$ cluster=cluster1, occurrence = 49    conf:(1)
  \item [$*$] sepallength='(5.5-6.1]' petallength='(3.95-4.933333]' ==$>$ cluster=cluster2, occurrence = 21    conf:(1)
  \item [$*$] petallength='(3.95-4.933333]' petalwidth='(0.9-1.3]' ==$>$ cluster=cluster2, occurrence = 18    conf:(1)
  \item [$*$] sepallength='(6.1-6.7]' petallength='(4.933333-5.916667]' ==$>$ cluster=cluster3, occurrence = 20    conf:(1)
  \item [$*$] sepalwidth='(2.8-3.2]' petallength='(4.933333-5.916667]' ==$>$ cluster=cluster3, occurrence = 18    conf:(1)
\end{enumerate}
From the association rules above, we infer that cluster1 is characterized by only petal length and petal width, while
cluster2 and cluster3 are characterized by mix of attribute values. This clustering is not good because there is 20.6667\% 
of incorrectly clustered instances. This is because some of the values don't reach the minimum support and don't
give rise to correct rules. From this we can conclude that instead of increasing the bins, we can perform discretize
in an efficient way.
\par
\textbf{\underline{Different Number of Clustering Algorithms}} \par
We started with \textbf{\textit{Hierarchical Clustering}} with 3 binned data, and got the below confusion matrix and 
incorrect clustered instance;
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{Hierarchical_Clustering.png}
  \captionof{figure}{Hierarchical clustering with 3 bins}
\end{center}
When compared Hierarchical clustering with SimpleKmeans, we could see that there is an increase in the percentage
of incorrectly clustered data points. In theory, Hierarchical clustering works by creating clusters for \textbf{N}
datapoints, so we will have \textbf{N} clusters and then combine them based on similarity measure. The similarity 
between the clusters is often calculated from the dissimilarity measures like the euclidean distance between two 
clusters. So the larger the distance between clusters, the better it is. So the reason for bad performance
could be distance between clusters would have been lesser, hence data points were not clustered properly.
\par
Then we started with \textbf{\textit{Filtered Clustering}}, we could see that results were same as the results of
SimpleKmeans algorithm and there was a swap of datapoints between clusters for species Iris-virginica and Iris-versicolor,
but Iris setosa was clustered properly.
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{Filtered_Clustering.png}
  \captionof{figure}{Filtered clustering with 3 bins}
\end{center}
\par
\textbf{\underline{Different Number of Clusters}} \par
From the dataset, we could see that it has only 3 classes, hence we can have 3 clusters. But, if we want to have 
more than the specified number of clusters, there is going to be further subdivision or partition in the data and 
some data points are going to be assigned to clusters which have no class names.
\newpage
We started our experiment with 6 clusters, and found the following results;
\begin{center}
  \includegraphics[width=80mm,scale=0.10]{kmeans_clustering_2.png}
  \captionof{figure}{SimpleKmeans clustering with 3 bins and 6 clusters}
\end{center}
We performed Apriori algorithm on data with 6 clusters and found association rules only for clusters 0,1, and 4
and not for clusters 2,3 and 5 with \textit{numrules} = 50 ,despite giving minimum confidence. Also, we could see that incorrectly clustered 
data points are more with 6 clusters because minimum confidence was not attained as well as some points of same 
class belongs to the larger clusters. From this information, we can infer that it is better to use 3 class 
partition to separate the data points very accurately.
\end{document}

